#!/usr/bin/env python3
"""
Standardizes GISAID strain names to our expected patterns.
"""
import argparse
import csv
import re
from sys import stdin
from typing import Iterable, Optional
from augur.io.json import dump_ndjson, load_ndjson
from augur.io.print import print_err


EGG_PASSAGE = "egg"
EGG_SUFFIX = "-egg"
EXPECTED_TYPES = {"a", "b"}


def standardize_record_strains(records: Iterable,
                               gisaid_strain_field: str,
                               passage_field: str,
                               type_field: str,
                               new_strain_field: str,
                               strain_replacements_file: Optional[str],
                               location_replacements_file: Optional[str]) -> Iterable:
    """
    Adds the *new_strain_field* to the *records*, with standardized strain name
    that was created from the *gisaid_strain_field*.

    Yields the modified records.
    """
    strain_replacements = {}
    location_replacements = {}
    if strain_replacements_file:
        strain_replacements = define_strain_fixes(strain_replacements_file)
    if location_replacements_file:
        location_replacements = define_location_label_fixes(location_replacements_file)

    for record in records:
        record = record.copy()
        gisaid_strain = record.get(gisaid_strain_field)
        passage_category = record.get(passage_field)
        influenza_type = record.get(type_field)

        if gisaid_strain is None:
            raise Exception(f"Records must have the expected GISAID strain field: {gisaid_strain_field!r}")

        if passage_category is None:
            raise Exception(f"Records must have the expected passage category field: {passage_field!r}")

        new_strain = fix_name(gisaid_strain, strain_replacements, location_replacements)

        # Mirrors the changes to the strain name in nextstrain/fauna/vdb/download.py
        # <https://github.com/nextstrain/fauna/blob/fbdc393581b1859ac817403d8f43e114d7edbc60/vdb/download.py#L326-L327>
        if passage_category == EGG_PASSAGE:
            new_strain += EGG_SUFFIX

        # Correct type prefix in strain name if it doesn't match the optional type field
        # Similar to how nextstrain/fauna/tdb/download.py corrects strain names
        # <https://github.com/nextstrain/fauna/blob/fbdc393581b1859ac817403d8f43e114d7edbc60/tdb/download.py#L99-L129>
        if influenza_type is not None and str(influenza_type).lower() in EXPECTED_TYPES:
            strain_type = new_strain.split("/")[0]
            if strain_type.lower() != influenza_type.lower():
                new_strain = re.sub("^" + re.escape(strain_type), influenza_type.upper(), new_strain)

        record[new_strain_field] = new_strain

        yield record


def define_strain_fixes(fname: str) -> dict:
    """
    Open strain name fixing files and define corresponding dictionaries

    Modified from nextstrain/fauna/vdb/upload.py
    <https://github.com/nextstrain/fauna/blob/3177b80bf010d9f75b449dbb400890af9bd920f8/vdb/upload.py#L142-L150>
    """
    reader = csv.DictReader(filter(lambda row: row[0]!='#', open(fname)), delimiter='\t')
    fix_whole_name = {}
    for line in reader:
        fix_whole_name[line['label'].encode().decode('unicode-escape')] = line['fix']
    return fix_whole_name


def define_location_label_fixes(fname: str) -> dict:
    """
    Open strain name location label fixing file and return dict of
    location label fixes.

    Modified from nextstrain/fauna/vdb/flu_upload.py
    <https://github.com/nextstrain/fauna/blob/3177b80bf010d9f75b449dbb400890af9bd920f8/vdb/flu_upload.py#L240-L244>
    """
    reader = csv.DictReader(filter(lambda row: row[0]!='#', open(fname)), delimiter='\t')
    label_to_fix = {}
    for line in reader:
        label_to_fix[line['label'].encode().decode('unicode-escape').replace(' ', '').lower()] = line['fix']
    return label_to_fix


def fix_name(name: str, fix_whole_name: dict, label_to_fix: dict) -> str:
    """
    Fix strain names

    Modified from nextstrain/fauna/vdb/flu_upload.py
    <https://github.com/nextstrain/fauna/blob/3177b80bf010d9f75b449dbb400890af9bd920f8/vdb/flu_upload.py#L256-L283>
    """
    # replace all accents with ? mark
    original_name = name.encode('ascii', 'replace').decode('unicode-escape')
    # Replace whole strain names
    name = replace_strain_name(original_name, fix_whole_name)
    # Reformat names like H1N1/Spain/PV-HUD-96039629/2025 to A/Spain/PV-HUD-96039629/2025
    if re.match(r'(H1N1|H3N2)/([^/]+/[^/]+/[0-9]{4})', name):
        match = re.match(r'(H1N1|H3N2)/([^/]+/[^/]+/[0-9]{4})', name)
        name = "A/" + match.group(2)
    name = name.replace('H1N1', '').replace('H5N6', '').replace('H3N2', '').replace('Human', '')\
        .replace('human', '').replace('//', '/').replace('.', '').replace(',', '').replace('&', '').replace(' ', '')\
        .replace('\'', '').replace('>', '').replace('-like', '').replace('+', '')
    split_name = name.split('/')
    # check location labels in strain names for fixing
    for index, label in enumerate(split_name):
        if label.replace(' ', '').lower() in label_to_fix:
            split_name[index] = label_to_fix[label.replace(' ', '').lower()]
    name = '/'.join(split_name)
    name = flu_fix_patterns(name)

    # Strip leading zeroes, change all capitalization location field to title case
    split_name = name.split('/')
    if len(split_name) == 4:
        if split_name[1].isupper() or split_name[1].islower():
            split_name[1] = split_name[1].title()  # B/WAKAYAMA-C/2/2016 becomes B/Wakayama-C/2/2016
        split_name[2] = split_name[2].lstrip('0')  # A/Mali/013MOP/2015 becomes A/Mali/13MOP/2015
        split_name[3] = split_name[3].lstrip('0')  # A/Cologne/Germany/01/2009 becomes A/Cologne/Germany/1/2009
    result_name = '/'.join(split_name).strip()
    return result_name


def replace_strain_name(original_name: str, fixes: dict = {}) -> str:
    """
    return the new strain name that will replace the original

    Modified from nextstrain/fauna/vdb/upload.py
    <https://github.com/nextstrain/fauna/blob/3177b80bf010d9f75b449dbb400890af9bd920f8/vdb/upload.py#L172-L179>
    """
    if original_name in fixes:
        return fixes[original_name]
    else:
        return original_name


def flu_fix_patterns(name: str) -> str:
    """
    Modified from nextstrain/fauna/vdb/flu_upload.py
    <https://github.com/nextstrain/fauna/blob/3177b80bf010d9f75b449dbb400890af9bd920f8/vdb/flu_upload.py#L285-L368>
    """
    # various name patterns that need to be fixed
    # capitalization of virus type
    if re.match(r'([a|b])([\w\s\-/]+)', name):  #b/sydney/508/2008    B/sydney/508/2008
        name = re.match(r'([a|b])([\w\s\-/]+)', name).group(1).upper() + re.match(r'([a|b])([\w\s\-/]+)', name).group(2)
    # remove inner parentheses and their contents
    if re.match(r'([^(]+)[^)]+\)(.+)', name):  # A/Egypt/51(S)/2006
        name = re.match(r'([^(]+)[^)]+\)(.+)', name).group(1) + re.match(r'([^(]+)[^)]+\)(.+)', name).group(2)
    # remove ending parentheses and their contents
    if re.match(r'([^(]+)[^)]+\)$', name):  # A/Eskisehir/359/2016 (109) -> A/Eskisehir/359/2016 ; A/South Australia/55/2014  IVR145  (14/232) -> A/South Australia/55/2014  IVR145
        name = re.match(r'([^(]+)[^)]+\)$', name).group(1)
    # Add year info to these Hongkong sequences
    if re.match(r'A/HongKong/H090-[0-9]{3}-V[0-9]$', name):  # A/HongKong/H090-750-V1 All confirmed from 2009
        name = name + "/2009"
    # Reformat names like A/NorthAmerica/Canada/NewBrunswick/NBPHLFLU03K-flu-MM00014R/2023 to A/NewBrunswick/NBPHLFLU03K-flu-MM00014R/2023
    if re.match(r'([A|B]/)NorthAmerica/Canada/(NewBrunswick/[^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/)NorthAmerica/Canada/(NewBrunswick/[^/]+/[0-9]{4})', name)
        name = match.group(1) + match.group(2)
    # Reformat names like A/ABUDHABI/UAE/0015851/2023 to A/ABUDHABI/0015851/2023
    if re.match(r'([A|B]/[^/]+/)UAE/([^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/[^/]+/)UAE/([^/]+/[0-9]{4})', name)
        name = match.group(1) + match.group(2)
    # Reformat names like A/St.Peterburg/CRIE/142/2024 to A/St.Peterburg/142/2024
    if re.match(r'([A|B]/[^/]+/)CRIE/([^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/[^/]+/)CRIE/([^/]+/[0-9]{4})', name)
        name = match.group(1) + match.group(2)
    # Reformat names like A/Uganda/UVRI_NISS-UV-0736_2024 to A/Uganda/UVRI_NISS-UV-0736/2024
    # and A/Uganda/UVRI_FTL2657_2024 to A/Uganda/UVRI_FTL2657/2024
    if re.match(r'([A|B]/Uganda/UVRI_[^/^_]+)_(2024)', name):
        match = re.match(r'([A|B]/Uganda/UVRI_[^/^_]+)_(2024)', name)
        name = match.group(1) + '/' + match.group(2)
    # Reformat names like A/La/EVTL-23435/2025 to A/Louisiana/EVTL-23435/2025
    if re.match(r'([A|B]/)LA(/EVTL-[^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/)LA(/EVTL-[^/]+/[0-9]{4})', name)
        name = match.group(1) + "Louisiana" + match.group(2)
    # Reformat names like A/Unknown/DE-DHSS-871/2025 to A/UnitedStates/DE-DHSS-871/2025
    if re.match(r'([A|B]/)Unknown(/DE-DHSS-[^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/)Unknown(/DE-DHSS-[^/]+/[0-9]{4})', name)
        name = match.group(1) + "UnitedStates" + match.group(2)
    # Reformat names like A/NJ/DE-DHSS-864/2025 to A/NewJersey/DE-DHSS-864/2025
    if re.match(r'([A|B]/)NJ(/DE-DHSS-[^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/)NJ(/DE-DHSS-[^/]+/[0-9]{4})', name)
        name = match.group(1) + "NewJersey" + match.group(2)
    # Reformat names like A/FL/DE-DHSS-874/2025 to A/Florida/DE-DHSS-874/2025
    if re.match(r'([A|B]/)FL(/DE-DHSS-[^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/)FL(/DE-DHSS-[^/]+/[0-9]{4})', name)
        name = match.group(1) + "Florida" + match.group(2)
    # Reformat names like A/AL/DE-DHSS-890/2025 to A/Alabama/DE-DHSS-890/2025
    if re.match(r'([A|B]/)AL(/DE-DHSS-[^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/)AL(/DE-DHSS-[^/]+/[0-9]{4})', name)
        name = match.group(1) + "Alabama" + match.group(2)
    # Reformat names like A/Lyon/CHU/0241865678/2024 to A/Lyon/0241865678/2024
    if re.match(r'([A|B]/Lyon/)CHU/([^/]+/[0-9]{4})', name):
        match = re.match(r'([A|B]/Lyon/)CHU/([^/]+/[0-9]{4})', name)
        name = match.group(1) + match.group(2)
    # Reformat names like A/165/Hungary/2025 to A/Hungary/165/2025
    if re.match(r'([A|B]/)([0-9]*/)Hungary/([0-9]{4})', name):
        match = re.match(r'([A|B]/)([0-9]*/)Hungary/([0-9]{4})', name)
        name = match.group(1) + "Hungary/" + match.group(2) + match.group(3)
    # Add year info to these Sendai sequences
    if re.match(r'A/Sendai/TU[0-9]{2}', name): # A/Sendai/TU08 All confirmed from 2010
        name = name + "/2010"
    # reformat names with clinical isolate in names, Philippines and Thailand
    if re.match(r'([A|B]/)clinicalisolate(SA[0-9]+)([^/]+)(/[0-9]{4})', name):  #B/clinicalisolateSA116Philippines/2002 -> B/Philippines/SA116/2002
        match = re.match(r'([A|B]/)clinicalisolate(SA[0-9]+)([^/]+)(/[0-9]{4})', name)
        name = match.group(1) + match.group(3) + "/" + match.group(2) + match.group(4)
    # reformat Ireland strain names
    if re.match(r'([1-2]+)IRL([0-9]+)$', name):  # 12IRL26168 -> A/Ireland/26168/2012  (All sequences with same pattern are H3N2)
        name = "A/Ireland/" + re.match(r'([1-2]+)IRL([0-9]+)$', name).group(2) + "/20" + re.match(r'([1-2]+)IRL([0-9]+)$', name).group(1)
    # Remove info B/Vic strain info from name
    if re.match(r'([\w\s\-/]+)(\(?)(B/Victoria/2/87|B/Victoria/2/1987)$', name):  # B/Finland/150/90 B/Victoria/2/1987 -> B/Finland/150/90
        name = re.match(r'([\w\s\-/]+)(\(?)(B/Victoria/2/87|B/Victoria/2/1987)$', name).group(1)
    # Separate location info from ID info in strain name
    if re.match(r'([A|B]/[^0-9/]+)([0-9]+[A-Za-z]*/[0-9/]*[0-9]{2,4})$', name):  #A/Iceland183/2009  A/Baylor4A/1983  A/Beijing262/41/1994
        name = re.match(r'([A|B]/[^0-9/]+)([0-9]+[A-Za-z]*/[0-9/]*[0-9]{2,4})$', name).group(1) + "/" + re.match(r'([A|B]/[^0-9/]+)([0-9]+[A-Za-z]*/[0-9/]*[0-9]{2,4})$', name).group(2)
    # Remove characters after year info, associated with passage info but can parse that from passage field later
    if re.match(r'([A|B]/[A-Za-z-]+/[A-Za-z0-9_-]+/[0-9]{4})(.)+$', name):  # B/California/12/2015BX59B A/Shanghai/11/1987/X99/highyieldingreassortant
        name = re.match(r'([A|B]/[A-Za-z-]+/[A-Za-z0-9_-]+/[0-9]{4})(.)+$', name).group(1)
    # Strip trailing slashes
    name = name.rstrip('/')  # A/NorthernTerritory/60/68//  A/Paris/455/2015/
    # Change two digit years to four digit years
    if re.match(r'([\w\s\-/]+)/([0-9][0-9])$', name):  #B/Florida/1/96 -> B/Florida/1/1996
        year = re.match(r'([\w\s\-/]+)/([0-9][0-9])$', name).group(2)
        if int(year) < 66:
            name = re.match(r'([\w\s\-/]+)/([0-9][0-9])$', name).group(1) + "/20" + year
        else:
            name = re.match(r'([\w\s\-/]+)/([0-9][0-9])$', name).group(1) + "/19" + year
    return name


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument("--strain-field", default="gisaid_strain",
        help="The record field containing the GISAID strain name")
    parser.add_argument("--passage-field", default="passage_category",
        help="The record field containing the passage category of the sample. " + \
             f"If the passage category is {EGG_PASSAGE!r}, then {EGG_SUFFIX!r} will be appended to the strain.")
    parser.add_argument("--type-field", default="vtype",
        help="The optional record field containing the Influenza type (e.g. a or b).")
    parser.add_argument("--new-strain-field", default="strain",
        help="The name of the new field to add to the record with the standardized strain name")
    parser.add_argument("--strain-replacements",
        help="A TSV file of full strain name replacements. " + \
             "Strains in the 'label' column are replaced with the strains in the 'fix' column.")
    parser.add_argument("--location-replacements",
        help="A TSV file of location label replacements in strain names. " + \
             "Location labels in the 'label' column are placed with the location in the 'fix' column. ")

    args = parser.parse_args()

    records = load_ndjson(stdin)
    modified_records = standardize_record_strains(
        records,
        args.strain_field,
        args.passage_field,
        args.type_field,
        args.new_strain_field,
        args.strain_replacements,
        args.location_replacements)
    dump_ndjson(modified_records)
